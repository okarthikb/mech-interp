{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Anthropic paper explainer notebook\n",
    "\n",
    "We discuss this [paper](https://transformer-circuits.pub/2021/framework/index.html#d-footnote-15) in this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "from einops import rearrange, reduce, repeat\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll be using the same hyperparams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = 32  # embedding dimension\n",
    "v = 128  # vocabulary size\n",
    "l = 64  # sequence length\n",
    "h = 4  # no. of heads\n",
    "# dh = d // h  # head dimension"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Standard interpretation of attention\n",
    "\n",
    "We have a matrix $t \\in \\mathbb{R}^{l \\times v}$ of $l$ one-hot vectors where $l$ is the sequence length. We have an embedding matrix $W_E \\in \\mathbb{R}^{v \\times d}$ where $d$ is the embedding dimension. $tW_E$ gets us the embedding vectors $x$ to input into the model, and $x = tW_E \\in \\mathbb{R}^{l \\times d}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([64, 32])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# nn.Embedding : torch.long -> torch.float\n",
    "# accepts a tensor of token ids and maps each to an embedding\n",
    "We = nn.Embedding(v, d)\n",
    "\n",
    "# example\n",
    "tokens = torch.randint(0, v, (l,))  # shape (l,)\n",
    "x = We(tokens)  # shape (l, d) = (64, 32)\n",
    "x.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we have the query, key, value, and atention output projection matrices $W_q, W_k, W_v, W_o \\in \\mathbb{R}^{d \\times d}$. We also choose the number of heads $h$ and $h \\vert d$. We get $q, k,$ and $v$ by doing the projections\n",
    "\n",
    "$\\{q, k, v\\} = xW_{\\{q, k, v\\}}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([64, 32]), torch.Size([64, 32]), torch.Size([64, 32]))"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Wq, Wk, Wv, Wo = [torch.randn(d, d) for _ in range(4)]\n",
    "\n",
    "q, k, v = x @ Wq, x @ Wk, x @ Wv  # (l, d) @ (d, d) = (l, d)\n",
    "\n",
    "q.shape, k.shape, v.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "and then we split $Q, K,$ and $V$ along the embedding dimension (i.e., the second dimension) for multi-head attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/karthik/Desktop/environments/chat/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(torch.Size([4, 64, 8]), torch.Size([4, 64, 8]), torch.Size([4, 64, 8]))"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "qs, ks, vs = map(\n",
    "  lambda m: rearrange(m, 'l (h dh) -> h l dh', h=h),\n",
    "  (q, k, v)\n",
    ")\n",
    "\n",
    "qs.shape, ks.shape, vs.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visually, we did this\n",
    "\n",
    "<img src=\"head_split.png\" height=384, width=608>\n",
    "\n",
    "Now we transpose all the key matrices like so\n",
    "\n",
    "<img src=\"key_transpose.png\" height=144 width=1120>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 8, 64])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ks = rearrange(ks, 'h l dh -> h dh l')\n",
    "ks.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we do a simple mat mul b/w all the query matrices and key matrices and take softmax along the last dimension to get the attention score matrices\n",
    "\n",
    "<img src=\"attention_.png\" height=960 width=784>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We must add a causal mask to our query key dot product matrix before softmax-ing it. We create a lower triangular matrix of 1s, subtract one to turn the 1s to 0s and the zeros above the diagonal to -1, then we replace the -1s with -inf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([64, 64])\n",
      "tensor([[0., -inf, -inf,  ..., -inf, -inf, -inf],\n",
      "        [0., 0., -inf,  ..., -inf, -inf, -inf],\n",
      "        [0., 0., 0.,  ..., -inf, -inf, -inf],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., -inf, -inf],\n",
      "        [0., 0., 0.,  ..., 0., 0., -inf],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]])\n"
     ]
    }
   ],
   "source": [
    "m = torch.tril(torch.ones(l, l)) - 1\n",
    "m[m == -1] = float('-inf')\n",
    "print(m.shape)\n",
    "print(m)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now compute attention scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 64, 64])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scores = F.softmax(qs @ ks + m, -1)  # (h, l, dh) @ (h, dh, l) = (h, l, l)\n",
    "scores.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 600x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# there are h score matrices – shape (h, l, l) – one for each head\n",
    "# we visualize the first \n",
    "\n",
    "fig = plt.figure(figsize=(6, 6))\n",
    "plt.imshow(scores[0].detach().numpy(), cmap='gray')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we mat mul each attention score matrix $S_i = Q_iK_i$ in `scores` with the corresponding value matrix $V_i$ in `vs` and get the heads\n",
    "\n",
    "<img src=\"score_vs.png\" height=960 width=768>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 64, 8])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "heads = scores @ vs  # (h, l, l) @ (h, l, dh) = (h, l, dh)\n",
    "\n",
    "heads.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then concatenate the heads (first image) then do a mat mul with $W_o$ (second image) to get the final output\n",
    "\n",
    "<img src=\"attn_orig.png\" height=640 width=512>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# (h, l, dh) -> (l, d)\n",
    "# (l, d) @ (d, d) = (l, d)\n",
    "attention_original = rearrange(heads, 'h l dh -> l (h dh)') @ Wo\n",
    "\n",
    "x.shape == attention_original.shape == (l, d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-26.7898, -40.5536, -31.7684,  ...,  44.2484,  23.8010, -20.8165],\n",
       "        [ 20.4618,  55.5261,  -4.4693,  ...,  -7.6764, -43.5488, -84.5420],\n",
       "        [-40.5892, -57.2393, -41.2147,  ...,  11.1041,  24.3113,  49.8133],\n",
       "        ...,\n",
       "        [ 27.3018, -14.0709,  21.4640,  ..., -24.1097, -91.7262,  -7.5286],\n",
       "        [-19.8293, -17.5503,  10.3039,  ..., -55.9022, -64.8398,  34.3185],\n",
       "        [ 45.9931,  -5.1464,  18.5283,  ..., -44.9761, -15.5306,   8.1222]],\n",
       "       grad_fn=<MmBackward0>)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attention_original"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reinterpretation\n",
    "\n",
    "We leave the embedding projection as is. We split $W_{\\{q, k, v\\}} \\in \\mathbb{R}^{d \\times d}$ along the column $h$ times to get $h$ matrices $W_{\\{q, k, v\\}}^1, \\ldots, W_{\\{q, k, v\\}}^h\\in \\mathbb{R}^{d \\times \\lfloor\\frac{d}{h}\\rfloor}$ and split $W_o$ along the row $h$ times to get $W_o^1, \\ldots, W_o^h \\in \\mathbb{R}^{\\lfloor \\frac{d}{h} \\rfloor \\times d}$\n",
    "\n",
    "$W_{\\{q, k, v\\}}$ split like so\n",
    "\n",
    "<img src=\"wqkv_split.png\" height=180 width=468>\n",
    "\n",
    "$W_o$ is split like so\n",
    "\n",
    "<img src=\"wo_split.png\" height=180 width=720>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([4, 32, 8]), torch.Size([4, 32, 8]), torch.Size([4, 32, 8]))"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# splitting Wq, Wk, Wv along the column h times\n",
    "Wq, Wk, Wv = map(\n",
    "  lambda m: rearrange(m, 'd (h dh) -> h d dh', h=h),\n",
    "  (Wq, Wk, Wv)\n",
    ")  # (d, d) -> (h, d, dh)\n",
    "\n",
    "Wq.shape, Wk.shape, Wv.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 8, 32])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# splitting Wo along the row h times\n",
    "Wo = rearrange(Wo, '(h dh) d -> h dh d', h=h)  # (d, d) -> (h, dh, d)\n",
    "\n",
    "Wo.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We won't be computing `vs` here, because we want to show the QK and OV circuit distinction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([4, 64, 8]), torch.Size([4, 64, 8]))"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "qs, ks = x @ Wq, x @ Wk  # (l, d) @ (h, d, dh) = (h, l, dh)\n",
    "\n",
    "qs.shape, ks.shape "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have\n",
    "\n",
    "$H^i = \\text{softmax}(Q^i {K^i}^T) V^i W_o^i$\n",
    "\n",
    "where $H^i \\in \\mathbb{R}^{l \\times d}$. We know $V_i$ is $xW_v^i$ so we seperate $H_i$ as a product of two terms\n",
    "\n",
    "$H^i = \\text{softmax}(Q^i {K^i}^T)(xW_v^i)W_o^i = (\\text{softmax}(Q^i {K^i}^T)x)(W_v^i W_o^i)$\n",
    "\n",
    "where the left term in the product is the QK circuit (determines source token value that information gets transferred FROM) and the right term is the OV circuit (determines subspace of source token value and subspace of destination token value and transfers info from one to the other). Now, attention is just the sum of all $H_i$\n",
    "\n",
    "$A = \\sum_{i = 1}^h H_i$\n",
    "\n",
    "We are of course not computing the one by one, we are doing batched mat mul and a reduction here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 64, 32])\n",
      "torch.Size([4, 32, 32])\n",
      "torch.Size([4, 64, 32])\n"
     ]
    }
   ],
   "source": [
    "ks = rearrange(ks, 'h l dh -> h dh l')  # transpose last 2 dims of ks first\n",
    "\n",
    "# QK circuit\n",
    "qk = F.softmax(qs @ ks + m, -1) @ x # (h, l, dh) @ (h, dh, l) @ (l, d) = (h, l, d)\n",
    "print(qk.shape)\n",
    "\n",
    "# OV circuit\n",
    "ov = Wv @ Wo  # (h, d, dh) @ (h, dh, d) = (h, d, d)\n",
    "print(ov.shape)\n",
    "\n",
    "# compute H_i for all i\n",
    "Hs = qk @ ov  # (h, l, d) @ (h, d, d) = (h, l, d)\n",
    "print(Hs.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now do the reduction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attention_new = reduce(Hs, 'h l d -> l d', 'sum')  # (h, l, d) -> (l, d)\n",
    "\n",
    "x.shape == attention_new.shape == (l, d)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check if `attention_new` is equal to `attention_original`. We should expect `attention_new == attention_original` to return false in some positions because of the non-associative nature of doing mat mul in a computer, but if we manually inspect the values, they should look the same"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[False, False, False,  ..., False,  True, False],\n",
       "        [False,  True, False,  ..., False,  True, False],\n",
       "        [False, False, False,  ..., False,  True, False],\n",
       "        ...,\n",
       "        [ True, False, False,  ...,  True, False, False],\n",
       "        [False, False, False,  ...,  True, False, False],\n",
       "        [False, False, False,  ...,  True, False, False]])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attention_new == attention_original"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-26.7898, -40.5536, -31.7684,  ...,  44.2484,  23.8010, -20.8165],\n",
       "        [ 20.4618,  55.5261,  -4.4693,  ...,  -7.6764, -43.5488, -84.5420],\n",
       "        [-40.5892, -57.2393, -41.2147,  ...,  11.1041,  24.3113,  49.8133],\n",
       "        ...,\n",
       "        [ 27.3018, -14.0709,  21.4640,  ..., -24.1097, -91.7263,  -7.5286],\n",
       "        [-19.8293, -17.5503,  10.3039,  ..., -55.9022, -64.8398,  34.3185],\n",
       "        [ 45.9931,  -5.1464,  18.5283,  ..., -44.9761, -15.5306,   8.1222]],\n",
       "       grad_fn=<SumBackward1>)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attention_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-26.7898, -40.5536, -31.7684,  ...,  44.2484,  23.8010, -20.8165],\n",
       "        [ 20.4618,  55.5261,  -4.4693,  ...,  -7.6764, -43.5488, -84.5420],\n",
       "        [-40.5892, -57.2393, -41.2147,  ...,  11.1041,  24.3113,  49.8133],\n",
       "        ...,\n",
       "        [ 27.3018, -14.0709,  21.4640,  ..., -24.1097, -91.7262,  -7.5286],\n",
       "        [-19.8293, -17.5503,  10.3039,  ..., -55.9022, -64.8398,  34.3185],\n",
       "        [ 45.9931,  -5.1464,  18.5283,  ..., -44.9761, -15.5306,   8.1222]],\n",
       "       grad_fn=<MmBackward0>)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attention_original"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
